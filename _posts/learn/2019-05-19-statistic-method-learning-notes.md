---
layout: post
title: 李航《统计学习方法》笔记（更新中）
category: 学习
tags: statistics, machine learning, python
description: 
---

[TOC]





## 0 绪论

> 统计学习就是计算机系统通过运用数据及统计方法提高系统性能的机器学习。现在，当人们提及机器学习时，往往是指统计机器学习。
> <p align="right">——李航《统计学习方法》</p>

### 分类

- **监督学习（Supervised Learning）**：训练数据包含了类别信息，典型问题是分类（Classification）和回归（Regression），典型算法有Logistic Regression、BP神经网络和线性回归算法。
- **无监督学习（Unsupervised Learning）**：训练数据中不包含类别信息，典型的问题是聚类（Clustering），代表算法为K-Means、DBSCAN算法等。
- 半监督学习（Semi-Supervised Learning）：训练数据中有一部分数据包含类别信息，另一部分不包含类别信息，是监督学习和无监督学习的融合。其算法一般是在监督学习的算法上进行扩展，使之可以对未标注数据建模。
- 增强学习（Reinforcement Learning）：强调如何基于环境而行动，以取得最大化的预期利益。如Alpha Go在人机对战中通过对当前棋局的分析决定下一步的落子。

#### 监督学习

分类和回归算法的最主要的区别是，分类算法中的类别是离散的值，如广告点击问题中的类别是 {+1, -1}，分别表示点击和未点击；而回归算法中的类别是连续的值，如通过人的身高、体重、性别信息预测人的年龄（年龄是连续的正整数）。

##### 流程

#### 重点

+ **过拟合**（over-fitting）：学习时选择的模型所包含的参数过多，复杂度比真模型更高，以致于出现这一模型对已知数据预测得很好，但对未知数据预测得很差的现象。
+ 当模型的复杂度增大时，训练误差会逐渐减小并趋向于0；测试误差会先减小，达到最小值后又增大。
+ 泛化能力（generalization ability）：由该方法学习到的模型对未知数据的预测能力，是学习方法本质上重要的性质。
+ 泛化误差（generalization error）：学到的模型对未知数据预测的误差。
+ 学习方法的泛化能力分析旺旺是通过研究泛化误差的概率上界进行的，简称**泛化误差上界**（generalization error bound）。泛化误差上界具有两个性质，它是样本容量和假设空间容量的函数。样本容量增加，泛化误差上界趋于0；假设空间容量越大，模型越难学，泛化误差上界越大。

## 2

## 3 $k$近邻法

### 算法

输入：训练数据集

$$T=\lbrace(x_1,y_1),(x_2,y_2,\cdots,(x_N,y_N)\rbrace$$

其中，$x_i$是实例的特征向量，$y_i$是实例的类别，$i=1,2,...,N$

输出：实例$x$所属的类$y$

1. 根据给定的距离度量，在训练集$T$中找出与x最邻近的k个点，涵盖着k个点的邻域记作$N_k(x)$
2. 在$N_k(x)$中根据分类决策规则（如多数表决）决定$x$的类别$y$：

$$y=arg \ {max_s}$$



$k=1$时称为最近邻算法

$k$近邻法没有显式的学习过程

### 距离度量：$L_p$距离

设特征空间是n维实数向量空间，$x_i,x_j \in X, \quad x_i =(x_i^{(1)},x_i^{(2)},\cdots,x_i^{(n)})^T,\quad x_j =(x_j^{(1)},x_j^{(2)},\cdots,x_j^{(n)})^T$，

$x_i,x_j$的$L_p$距离定义为

$$L_p(x_i,x_j)=\bigg(\sum_{l=1}^n|x_i^{(l)}-x_j^{(l)}|^p \bigg) ^{\frac{1}{p}}$$

$p=1$时，称为曼哈顿距离

$p=2$时，称为欧氏距离

$p=\infty$时，它是各个坐标距离的最大值，即

$$L_\infty (x_i,x_j)=max|x_i^{(l)}-x_j^{(l)}|$$

### $k$值的选择

如果选择较小的$k$值，就相当于用较小的邻域中的训练实例进行预测。优点是“学习”的近似误差（approximation error）会减小，缺点是估计误差（estimation error）会增大，预测结果会对近邻的实例点非常敏感。即：**$k$值的减小意味着整体模型变得复杂，容易发生过拟合**。

如果选择较大的$k$值，就相当于用较大邻域中的训练实例进行预测。优点是可以减少估计误差，缺点是会增大近似误差。这时与输入实例较远的训练实例也会对预测起作用，使预测发生错误。即：$k$值的增大意味着整体模型变得简单。

实际应用中，$k$一般取一个较小的数值。通常采用交叉验证法选取最优的$k$值。

### 分类决策规则

一般采用多数表决规则（majority voting rule），它等价于经验风险最小化。

